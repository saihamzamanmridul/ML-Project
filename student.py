# -*- coding: utf-8 -*-
"""Student

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MTldxkt9PX0NY474JmsIDrPJLo1sD-IE
"""

file_path ='Student performance (Polytechnic Institute of Portalegre).csv'

"""**Student performance Dataset**


"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Importing libraries for statistical analysis
from scipy import stats

# Importing libraries for machine learning and model building
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from scipy.stats import ttest_ind
from statsmodels.formula.api import ols
import statsmodels.api as sm


from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from imblearn.over_sampling import RandomOverSampler
import matplotlib.pyplot as plt
import seaborn as sns

# Importing libraries for handling class imbalance
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import make_pipeline

# Importing libraries for model interpretability
import shap

df= pd.read_csv(file_path)

print("Dataset structure:")
print(df.head(10))

df.info()

print("\nMissing values:")
print(df.isnull().sum())

# Check for null values
null_counts = df.isnull().sum()

# Visualize null values
plt.figure(figsize=(10, 6))
sns.barplot(x=null_counts.index, y=null_counts.values, palette='coolwarm')
plt.title("Null Value Counts by Feature")
plt.xticks(rotation=90)
plt.ylabel("Count of Null Values")
plt.show()

# Display columns with null values
null_counts[null_counts > 0]

print("Descriptive Statistics:")
print(df.describe())

# Visualize relationships between features and the target variable ('Target')
# Example: Histogram of the 'Target' variable
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Target', palette='viridis')
plt.title("Target Variable Distribution")
plt.show()

# Scatter plot of 'Curricular units 1st sem (grade)' vs. 'Curricular units 2nd sem (grade)' colored by 'Target'
plt.figure(figsize=(10, 8))
sns.scatterplot(x='Curricular units 1st sem (grade)', y='Curricular units 2nd sem (grade)', hue='Target', data=df)
plt.title('1st Semester Grade vs. 2nd Semester Grade')
plt.show()

plt.figure(figsize=(12, 10))
corr_matrix = df[numerical_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

df.columns = df.columns.str.strip().str.replace('[^a-zA-Z0-9]', '_')

# Perform t-tests to analyze the relationship between 'Previous qualification (grade)' and 'Daytime/evening attendance'
# Split data into groups based on 'Daytime/evening attendance'
daytime_grades = df[df['Daytime/evening attendance'] == 1]['Previous qualification (grade)']
evening_grades = df[df['Daytime/evening attendance'] == 0]['Previous qualification (grade)']

# Perform t-test to compare mean grades between daytime and evening students
t_stat, p_value = ttest_ind(daytime_grades, evening_grades, equal_var=False)
print(f"T-test results for Previous qualification (grade) between Daytime and Evening students:")
print(f"T-statistic: {t_stat:.4f}, p-value: {p_value:.4f}")

# Perform ANOVA to analyze the effect of 'Marital status' on 'Previous qualification (grade)'
anova_model = ols('Q("Previous qualification (grade)") ~ C(Q("Marital status"))', data=df).fit()
anova_table = sm.stats.anova_lm(anova_model, typ=2)
print("\nANOVA results for Marital status and Previous qualification (grade):")
print(anova_table)

# Separate features (X) and target variable (y)
X = df.drop(columns=['Target'])  # Features
y = df['Target']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize numerical features (if needed)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle class imbalance using RandomOverSampler
oversampler = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_scaled, y_train)

# Select and train machine learning models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier()
}

for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train_resampled, y_train_resampled)

    # Evaluate model performance on test set
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"\n{name} Model Evaluation:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(f"Confusion Matrix:\n{conf_matrix}")
    print(classification_report(y_test, y_pred))
 # Visualize confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Separate features (X) and target variable (y)
X = df.drop(columns=['Target'])  # Features
y = df['Target']  # Target variable

# One-hot encode the target variable for multi-class classification
y_encoded = to_categorical(y)  # Converts the target to one-hot encoded format

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle class imbalance using RandomOverSampler
oversampler = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_scaled, y_train.argmax(axis=1))

# Convert y_train_resampled back to one-hot encoded format
y_train_resampled = to_categorical(y_train_resampled)

# Build the ANN model
model = Sequential([
    Dense(64, input_dim=36, activation='relu'),  # Input layer with 64 neurons
    Dense(32, activation='relu'),               # Hidden layer with 32 neurons
    Dense(3, activation='softmax')              # Output layer with 3 neurons (for 3 classes)
])

# Compile the ANN
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the ANN
history = model.fit(X_train_resampled, y_train_resampled,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=1)

# Evaluate the ANN on the test set
y_pred_probs = model.predict(X_test_scaled)
y_pred = y_pred_probs.argmax(axis=1)
y_test_labels = y_test.argmax(axis=1)

accuracy = accuracy_score(y_test_labels, y_pred)
conf_matrix = confusion_matrix(y_test_labels, y_pred)

print(f"\nANN Model Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test_labels, y_pred))

# Visualize confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("ANN Confusion Matrix")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Separate features (X) and target variable (y)
X = df.drop(columns=['Target'])  # Features
y = df['Target']  # Target variable

# One-hot encode the target variable for multi-class classification
y_encoded = to_categorical(y)  # Converts the target to one-hot encoded format

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle class imbalance using RandomOverSampler
oversampler = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_scaled, y_train.argmax(axis=1))

# Convert y_train_resampled back to one-hot encoded format
y_train_resampled = to_categorical(y_train_resampled)

# Build the improved ANN model with 3 hidden layers (each having 36 neurons)
model = Sequential([
    Dense(36, input_dim=36, activation='relu'),
    Dense(36, activation='relu'),
    Dense(36, activation='relu'),
    Dense(3, activation='softmax')
])

# Compile the ANN
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the ANN
history = model.fit(X_train_resampled, y_train_resampled,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=1)

# Evaluate the ANN on the test set
y_pred_probs = model.predict(X_test_scaled)
y_pred = y_pred_probs.argmax(axis=1)
y_test_labels = y_test.argmax(axis=1)

accuracy = accuracy_score(y_test_labels, y_pred)
conf_matrix = confusion_matrix(y_test_labels, y_pred)

print(f"\nImproved ANN Model Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test_labels, y_pred))

# Visualize confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("ANN Confusion Matrix")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

data = pd.read_csv(file_path)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Encode the target variable
target_column = 'Target'  # Update this to match your dataset's target column name
le_target = LabelEncoder()
data[target_column] = le_target.fit_transform(data[target_column])

# Separate features and target
X = data.drop(columns=[target_column])
y = data[target_column]

# Encode categorical features
label_encoders = {}
for col in X.select_dtypes(include='object').columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Scale numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Remove outliers using z-score
z_scores = np.abs((X_scaled - np.mean(X_scaled, axis=0)) / np.std(X_scaled, axis=0))
threshold = 3
outlier_indices = np.where(z_scores > threshold)
rows_with_outliers = set(outlier_indices[0])

# Filter out rows with outliers
X_filtered = np.delete(X_scaled, list(rows_with_outliers), axis=0)
y_filtered = np.delete(y.values, list(rows_with_outliers), axis=0)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)

# Train the model
rf_classifier = RandomForestClassifier(random_state=42, n_estimators=100)
rf_classifier.fit(X_train, y_train)

# Make predictions
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=le_target.classes_)

# Display results
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)

import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# List of classifiers to compare
classifiers = {
    'Logistic Regression': LogisticRegression(),
    'SVM': SVC(),
    'KNN': KNeighborsClassifier(),
    'Random Forest': rf_classifier,  # Already trained
    'Gradient Boosting': GradientBoostingClassifier()
}

# Initialize a list to store accuracy scores
accuracy_scores = []

# Train and evaluate each model
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)
    print(f"{name} Accuracy: {accuracy * 100:.2f}%")

# Plot comparison of model accuracies
plt.figure(figsize=(10, 6))
plt.bar(classifiers.keys(), accuracy_scores, color='skyblue')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Model Comparison')
plt.ylim(0, 1)
plt.show()

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Encode the target variable
le = LabelEncoder()
df['Target'] = le.fit_transform(df['Target'])

# Split the dataset
X = df.drop('Target', axis=1)
y = df['Target']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

# Visualize balanced target variable
plt.figure(figsize=(8, 6))
sns.countplot(x=y_train_bal, palette='coolwarm')
plt.title("Balanced Target Distribution (After SMOTE)")
plt.xticks([0, 1, 2], ['Dropout', 'Graduate', 'Graduate'])
plt.show()

# Import models and evaluation libraries
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=500),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

# Train and evaluate models
train_accuracies = []
test_accuracies = []

for model_name, model in models.items():
    # Train the model
    model.fit(X_train_bal, y_train_bal)

    # Evaluate on train and test sets
    train_acc = accuracy_score(y_train_bal, model.predict(X_train_bal))
    test_acc = accuracy_score(y_test, model.predict(X_test))

    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)

    # Display results
    print(f"{model_name}:")
    print(f"  Train Accuracy: {train_acc:.2f}")
    print(f"  Test Accuracy:  {test_acc:.2f}")
    print("-" * 30)

# Define models_list
models_list = list(models.keys())  # Get model names from the 'models' dictionary

# Visualize the results
x = np.arange(len(models_list))  # the label locations
width = 0.35  # the width of the bars

# Create a bar chart for train and test accuracies
fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - width/2, train_accuracies, width, label='Train Accuracy', color='skyblue')
rects2 = ax.bar(x + width/2, test_accuracies, width, label='Test Accuracy', color='orange')

# Add text labels, title and custom x-axis tick labels, etc.
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy')
ax.set_title('Model Comparison: Train vs Test Accuracy')
ax.set_xticks(x)
ax.set_xticklabels(models_list, rotation=45)  # Use models_list for labels
ax.legend()